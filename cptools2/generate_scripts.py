"""
Generate eddie submission scripts for:
- Staging: Moving input data (e.g., images) from a storage location to the cluster's compute nodes.
- Analysis: Running the actual CellProfiler analysis on the staged data.
- Destaging: Moving the results generated by CellProfiler back to the storage location.
"""

from __future__ import print_function
import os
import textwrap
import subprocess
from datetime import datetime
import yaml
from scissorhands import script_generator
from cptools2 import utils
from cptools2 import colours
from cptools2.colours import pretty_print


def make_command_paths(commands_location):
    """
    create the paths to the commands
    """
    names = ["staging", "cp_commands", "destaging"]
    return {name: os.path.join(commands_location, name+".txt") for name in names}


def _lines_in_commands(staging, cp_commands, destaging):
    """
    Number of lines in each of the commands file.
    While the number of lines in each of the files *should* be the same,
    it's worth checking.

    Parameters:
    -----------
    staging: string
        path to staging commands

    cp_commands: string
        path to cellprofiler commands

    destaging: string
        path to destaging commands

    Returns:
    --------
    Dictionary, e.g:
        {staging: 128, cp_commands: 128, destaging: 128}
    """
    names = ["staging", "cp_commands", "destaging"]
    paths = [staging, cp_commands, destaging]
    counts = [utils.count_lines_in_file(i) for i in paths]
    # check if the counts differ
    if len(set(counts)) > 1:
        raise RuntimeWarning("command files contain differing number of lines")
    return {name: count for name, count in zip(names, counts)}


def lines_in_commands(commands_location):
    """
    Given a path to a directory which contains the commands:
        1. staging
        2. cellprofiler commands
        3. destaging
    This will return the number of lines in each of these text files.

    Parameters:
    -----------
    commands_location: string
        path to directory containing commands

    Returns:
    ---------
    Dictionary

        {"staging":     int,
         "cp_commands": int,
         "destaging":   int}
    """
    command_paths = make_command_paths(commands_location)
    return _lines_in_commands(**command_paths)


def load_module_text():
    """returns load module commands"""
    return textwrap.dedent(
        """
        module load anaconda/2024.02
        source activate cellprofiler
        """
    )


def make_qsub_scripts(commands_location, commands_count_dict, logfile_location):
    """
    Create and save qsub submission scripts in the same location as the
    commands.

    Parameters:
    -----------
    commands_location: string
        path to directory that contains staging, cp_commands, and destaging
        command files.

    commands_count_dict: dictionary
        dictionary of the number of commands contain in each of the jobs

    logfile_location: string
        where to store the log files. By default this will store them
        in a directory alongside the results.


    Returns:
    ---------
    Nothing, writes files to `commands_location`
    """
    cmd_path = make_command_paths(commands_location)
    time_now = datetime.now().replace(microsecond=0)
    time_now = str(time_now).replace(" ", "-")
    # append random hex to job names - this allows you to run multiple jobs
    # without the -hold_jid flags fron clashing
    job_hex = script_generator.generate_random_hex()
    n_tasks = commands_count_dict["cp_commands"]

    # --- PLACEHOLDERS: These should be configurable ---
    staging_concurrency_limit = 20
    analysis_concurrency_limit = 15
    analysis_cores = 8
    analysis_total_memory_gb = 16
    # --- End Placeholders ---

    # Staging Script (Single Core, Concurrency Limited)
    # FIXME: using AnalysisScript class for everything, due to the
    #        {Staging, Destaging}Script class not having loop_through_file
    stage_script = BodgeScript(
        name="staging_{}".format(job_hex),
        memory="1G", # Keep staging memory simple (adjust if needed)
        output=os.path.join(logfile_location, "staging"),
        tasks=commands_count_dict["staging"]
    )
    stage_script += "#$ -q staging\\n"
    # limit staging node requests
    stage_script += "#$ -p -500\\n"
    # Apply concurrency limit to staging
    stage_script += f"#$ -tc {staging_concurrency_limit}\\n"
    stage_script.bodge_array_loop(phase="staging",
                                  input_file=cmd_path["staging"])
    stage_loc = os.path.join(commands_location,
                             "{}_staging_script.sh".format(time_now))
    stage_script.save(stage_loc)

    # Analysis Script (Shared Memory Parallel, Concurrency Limited)
    # Calculate memory per core for SGE h_vmem requirement
    try:
        mem_per_core_gb = float(analysis_total_memory_gb) / analysis_cores
        if mem_per_core_gb < 0.1: # Avoid extremely small values
             mem_per_core_gb = 0.1
    except ZeroDivisionError:
        pretty_print("Error: analysis_cores cannot be zero.", colour="red")
        # Handle error appropriately, maybe raise an exception or default
        mem_per_core_gb = 1 # Default to 1G per core if cores is 0

    analysis_script = script_generator.AnalysisScript(
        name="analysis_{}".format(job_hex),
        tasks=n_tasks,
        hold_jid_ad="staging_{}".format(job_hex),
        pe=f"sharedmem {analysis_cores}", # Use shared memory environment
        memory=f"{mem_per_core_gb:.2f}G", # Memory per core
        output=os.path.join(logfile_location, "analysis")
    )
    analysis_script += load_module_text()
    # Apply concurrency limit to analysis
    analysis_script += f"#$ -tc {analysis_concurrency_limit}\\n"
    # Set OpenMP threads based on allocated slots
    analysis_script += "export OMP_NUM_THREADS=$NSLOTS\\n"
    analysis_script.loop_through_file(cmd_path["cp_commands"])
    analysis_loc = os.path.join(commands_location,
                                "{}_analysis_script.sh".format(time_now))
    analysis_script += make_logfile_text(logfile_location,
                                         job_file=job_hex,
                                         n_tasks=n_tasks)
    analysis_script.save(analysis_loc)
    destaging_script = BodgeScript(
        name="destaging_{}".format(job_hex),
        memory="1G", # Keep destaging memory simple
        hold_jid_ad="analysis_{}".format(job_hex),
        tasks=commands_count_dict["destaging"],
        output=os.path.join(logfile_location, "destaging")
    )
    # No concurrency limit or PE needed for standard destaging
    destaging_script.bodge_array_loop(phase="destaging",
                                      input_file=cmd_path["destaging"])
    destage_loc = os.path.join(commands_location,
                               "{}_destaging_script.sh".format(time_now))
    destaging_script.save(destage_loc)
    # create script to submit staging, analysis and destaging scripts
    submit_script = make_submit_script(commands_location, time_now)
    pretty_print("saving master submission script at {}".format(colours.yellow(submit_script)))
    utils.make_executable(submit_script)


def make_logfile_text(logfile_location, job_file, n_tasks):
    text = """
    # get the exit code from the cellprofiler job
    RETURN_VAL=$?

    if [[ $RETURN_VAL == 0 ]]; then
        RETURN_STATUS="Finished"
    else
        RETURN_STATUS="Failed with error code: $RETURN_VAL"
    fi

    LOG_FILE_LOC={logfile_location}/{job_file}.log
    echo "`date +"%Y-%m-%d %H:%M"`  "$JOB_ID"  "$SGE_TASK_ID"  "$RETURN_STATUS"" >> "$LOG_FILE_LOC"
    """.format(logfile_location=logfile_location,
               job_file=job_file,
               n_tasks=n_tasks)
    return textwrap.dedent(text)


def make_submit_script(commands_location, job_date):
    """
    Create a shell script which will submit the staging, analysis and
    destaging scripts.

    Parameters:
    -----------
    commands_location: string
        path to where the commands are stored
    job_date: string
        date for the submission scripts

    Returns:
    --------
    path to submit_script
    also writes script to disk in `commands_location`.
    """
    # create full paths to the generated scripts
    names = ["staging", "analysis", "destaging"]
    script_dict = {}
    for name in names:
        script_name = "{}_{}_script.sh".format(job_date, name)
        script_path = os.path.join(commands_location, script_name)
        script_dict[name] = script_path
    # create text for a shell script that qsub's the scripts, waiting for scratch space
    output = f"""
             #!/bin/sh

             # This script submits the staging, analysis and destaging
             # scripts in the correct order, waiting if scratch usage is high.

             # NOTE: run this as a shell script, NOT a submission script
             # so either call `./name_of_script.sh` or `bash name_of_script.sh`

             SCRATCH_THRESHOLD=90.0 # Percentage
             WAIT_SECONDS=300       # 5 minutes

             # --- Function to check scratch usage ---
             # This relies on 'python' and 'bc' being available in the PATH
             check_scratch() {{
               # Use python to run the check, printing percentage to stdout
               # We embed the logic here for simplicity, assuming the necessary
               # python modules (os, subprocess, sys) are available.
               python3 -c "
import subprocess
import os
import sys

def get_scratch_usage():
    user = os.environ.get('USER', None)
    if not user:
        print(f'Error: Could not determine USER environment variable.', file=sys.stderr)
        return 101 # Indicate error
    cmd = f'du -s --bytes /exports/eddie/scratch/{{user}} | cut -f1'
    try:
        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False, timeout=60)
        if result.returncode != 0 or not result.stdout.strip():
            print(f'Error running or parsing du command: {{result.stderr or 'No output'}}', file=sys.stderr)
            return 101 # Indicate error
        usage_bytes = int(result.stdout.strip())
        quota_bytes = 2 * (1024**4) # 2 TiB in bytes
        if quota_bytes == 0: return 0
        usage_percentage = (usage_bytes / quota_bytes) * 100
        return usage_percentage
    except ValueError:
        print(f'Error: Could not parse du output \\'{{result.stdout.strip() if 'result' in locals() else 'N/A'}}\\'', file=sys.stderr)
        return 101 # Indicate error
    except subprocess.TimeoutExpired:
        print(f'Error: du command timed out after 60 seconds.', file=sys.stderr)
        return 101 # Indicate error
    except Exception as e:
        print(f'Error checking scratch usage: {{e}}', file=sys.stderr)
        return 101 # Indicate error

usage = get_scratch_usage()
if usage >= 101: # Error case from function
    print(f'0.0') # Output 0 to avoid shell errors, error message printed to stderr
    sys.exit(1) # Exit python with error
else:
    print(f'{{usage:.2f}}') # Print percentage to stdout
    sys.exit(0) # Exit python successfully
"
             }}
             # --- End function ---

             echo "Checking initial scratch disk usage..."
             CURRENT_USAGE=$(check_scratch)
             EXIT_CODE=$?

             if [ $EXIT_CODE -ne 0 ]; then
                 echo "Error: Failed to get initial scratch usage. Python script failed. Exiting."
                 exit 1
             fi

             # Check if bc is available for float comparison
             if ! command -v bc > /dev/null 2>&1; then
                 echo "Error: 'bc' command not found, cannot perform floating point comparison for scratch threshold. Exiting."
                 exit 1
             fi

             # Loop while usage is above threshold
             # Use 'bc' for floating point comparison
             while [ $(echo "$CURRENT_USAGE > $SCRATCH_THRESHOLD" | bc) -eq 1 ]; do
               echo "Scratch usage ($CURRENT_USAGE%) exceeds threshold ($SCRATCH_THRESHOLD%). Waiting $WAIT_SECONDS seconds..."
               sleep $WAIT_SECONDS
               echo "Re-checking scratch disk usage..."
               CURRENT_USAGE=$(check_scratch)
               EXIT_CODE=$?
               if [ $EXIT_CODE -ne 0 ]; then
                   echo "Error: Failed to get scratch usage during wait loop. Python script failed. Exiting."
                   exit 1
               fi
             done

             echo "Scratch usage ($CURRENT_USAGE%) is acceptable. Submitting jobs..."

             # Submit the jobs
             qsub {script_dict["staging"]}
             qsub {script_dict["analysis"]}
             qsub {script_dict["destaging"]}

             echo "Jobs submitted."
             exit 0
            """
    save_location = "{}/{}_SUBMIT_JOBS.sh".format(commands_location, job_date)
    # save this shell script and return it's path
    with open(save_location, "w") as f:
        f.write(textwrap.dedent(output))
    return save_location


class BodgeScript(script_generator.AnalysisScript):
    """
    Whilst trying to fix rsync issues with filepaths containing spaces,
    the rsync commands stopped working when called from `$SEED`, though
    will work if saved as a single command in a shell script, and then calling
    bash on that script.

    So this class inherits scissorhands.script_generator.AnalsisScript, but
    adds an extra method which should be used instead of .loop_through_file().
    """

    def __init__(self, *args, **kwargs):
        script_generator.AnalysisScript.__init__(self, *args, **kwargs)

    def bodge_array_loop(self, phase, input_file):
        """
        As a temporary fix (hopefully), this method can work instead of
        scissorhands.script_generator.AnalysisScript.loop_through_file()

        Parameters:
        -----------
        phase: string
            prefix of the hidden commands file, e.g "staging" or "destaging"
        input_file: string
            path to a file. This file should contain multiple lines of commands.
            Each line will be run separately in an array job.

        Returns:
        ---------
        nothing, adds text to template
        """
        text = textwrap.dedent(
            """
            SEEDFILE="{input_file}"
            SEED=$(awk "NR==$SGE_TASK_ID" "$SEEDFILE")
            # create shell script from single command, run, then delete
            echo "$SEED" > .{phase}_"$JOB_ID"_"$SGE_TASK_ID".sh
            bash .{phase}_"$JOB_ID"_"$SGE_TASK_ID".sh
            rm .{phase}_"$JOB_ID"_"$SGE_TASK_ID".sh
            """.format(phase=phase, input_file=input_file)
        )
        self.template += text


def get_scratch_usage():
    """Returns scratch usage as a percentage of the 2TB quota (approx)."""
    user = os.environ.get('USER', None)
    if not user:
        pretty_print("Warning: Could not determine USER for scratch check.", colour="yellow")
        return 0 # Assume OK if user cannot be determined

    cmd = f"du -s --bytes /exports/eddie/scratch/{user} | cut -f1"
    try:
        result = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=False)
        if result.returncode != 0 or not result.stdout.strip():
            pretty_print(f"Warning: 'du' command failed or gave empty output. Cannot check scratch usage.\nStderr: {result.stderr}", colour="yellow")
            return 0 # Assume OK if check fails

        # Calculate usage in bytes and convert to percentage
        usage_bytes = int(result.stdout.strip())
        quota_bytes = 2 * (1024**4)  # 2TB in bytes (TiB)
        if quota_bytes == 0:
            return 0 # Avoid division by zero
        usage_percentage = (usage_bytes / quota_bytes) * 100

    except ValueError:
        pretty_print(f"Warning: Could not parse 'du' output ('{result.stdout.strip()}'). Cannot check scratch usage.", colour="yellow")
        return 0 # Assume OK if parsing fails
    except Exception as e:
        pretty_print(f"Warning: Error checking scratch usage: {e}", colour="yellow")
        return 0 # Assume OK on other errors

    return usage_percentage

